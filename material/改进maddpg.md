# 改进的MADDPG算法伪代码

### 网络结构

```markdown
class GraphAttention:
    输入: state特征, 邻接矩阵
    1. 计算注意力分数:
        - 对输入进行线性变换
        - 计算节点对之间的注意力系数
        - 使用softmax归一化注意力权重
    2. 输出: 聚合后的node特征

class Actor(GNN):
    1. 图注意力层1: state维度 -> 64维
    2. 图注意力层2: 64维 -> 32维 
    3. 全连接层: 32维 -> action维度
    4. 输出: softmax(action)

class Critic(GNN):
    1. 图注意力层1: (state维度 + action维度) -> 64维
    2. 图注意力层2: 64维 -> 32维
    3. 全连接层: 32维 -> 1维(Q值)
```

### 噪声策略

```markdown
class NoiseStrategy:
    1. OU噪声:
        - 用于动作空间探索
        - 参数: mu(均值), theta(均值漂移), sigma(扰动强度)
        
    2. 参数空间噪声:
        - 对网络参数添加高斯噪声
        - 自适应调整噪声标准差
        
    3. 噪声衰减:
        - 初始scale=1.0
        - 每步衰减: scale *= decay_rate
        - 最小scale=0.1
```

### 主算法流程

```markdown
初始化:
    1. 为每个智能体创建Actor和Critic网络及其目标网络
    2. 初始化经验回放缓冲区
    3. 初始化噪声生成器
    4. 初始化优化器

训练过程:
    For each episode:
        重置环境获得初始状态
        重置噪声过程
        
        For each step:
            1. 构建智能体交互图:
                - 计算智能体间距离
                - 生成邻接矩阵(距离小于阈值的智能体相连)
                
            2. 动作选择:
                For each agent:
                    - 使用Actor网络生成基础动作
                    - 添加OU噪声
                    - 添加参数空间噪声
                    - 裁剪动作到合法范围
                
            3. 环境交互:
                - 执行动作获得奖励和下一状态
                - 存储经验(state, action, reward, next_state, done, adj_matrix)
                
            4. 训练(当经验池足够时):
                For each agent:
                    a) 更新Critic:
                        - 计算目标Q值(使用目标网络)
                        - 计算当前Q值
                        - 最小化TD误差
                        
                    b) 更新Actor:
                        - 基于确定性策略梯度更新
                        
                    c) 软更新目标网络
                    
            5. 更新噪声参数:
                - 衰减噪声scale
                - 自适应调整参数噪声强度
```

### 关键改进点

1. **图神经网络结构**:
   - 使用图注意力机制捕捉智能体间的动态关系
   - 自适应加权聚合邻居信息

2. **混合噪声策略**:
   - 结合OU噪声和参数空间噪声
   - 动态调整噪声强度

3. **图构建策略**:
   - 基于距离阈值构建动态交互图
   - 使用距离的倒数作为边权重


