# GCN-MADDPG算法总结

## 概述
GCN-MADDPG（图卷积网络多智能体深度确定性策略梯度）是一种结合了图卷积网络（GCN）和多智能体深度确定性策略梯度（MADDPG）的方法，用于解决复杂的多智能体协作任务。本算法特别适用于涉及智能体之间复杂交互的场景，如人群疏散模拟中的多智能体协调问题。

## 算法架构

### 1. 环境 `CrowdEvacuationEnv`
- **状态空间**：每个智能体的位置坐标。
- **动作空间**：智能体的动作（如移动方向）。
- **奖励机制**：根据智能体与出口的接近程度和步数计算奖励。
- **重置与终止**：智能体从随机位置开始，并在达到出口或达到最大步数时终止。

### 2. 图卷积网络（GCN）
- **GraphConvolution**：用于提取图结构信息，通过邻接矩阵对节点特征进行卷积。

### 3. Actor和Critic网络
- **GCNActor**：用于策略生成，接收状态和邻接矩阵，输出每个智能体的动作概率分布。
- **GCNCritic**：用于价值评估，接收状态、动作及邻接矩阵，输出Q值。

### 4. 经验回放缓冲区 (`Memory`)
- 用于存储智能体的交互经验，以便进行经验重放，增强学习的样本多样性。

### 5. 训练与更新
- **选择动作**：使用策略（actor）和噪声机制生成动作。
- **存储经验**：将交互生成的经验存储到缓冲区。
- **更新网络**：从缓冲区中随机采样进行网络的更新，包括critic和actor的更新.
  - **Critic更新**：通过目标网络计算目标Q值，计算误差并进行梯度下降。
  - **Actor更新**：通过最大化Critic的输出进行策略更新。

### 6. 参数与噪声
- **参数更新**：使用软更新机制保持目标网络与主网络的一致性。
- **噪声机制**：包括Ornstein-Uhlenbeck噪声和参数空间噪声（ParamNoise），用于增强探索能力。

## 训练与评估
- **训练过程**：通过迭代多个episode进行训练，在每个episode中，智能体通过与环境交互不断调整策略。
- **评估过程**：在训练完成后，评估智能体的表现，计算平均奖励。

## 主函数与测试
- **主函数** (`main()`)：设置随机种子，创建环境与智能体，进行训练与评估。
- **测试不同场景**：测试不同配置的环境，验证模型的泛化能力。

## 可视化与分析
- **注意力权重可视化** (`GNNAnalyzer`, 改为 `GCNAnalyzer`)：通过分析和可视化GCN层的特征，以便了解智能体在交互过程中关注的重点。

## 结论
GCN-MADDPG通过引入图卷积网络，增强了智能体之间交互信息的捕捉与利用，适用于复杂多智能体环境下的任务协作与优化。

---
