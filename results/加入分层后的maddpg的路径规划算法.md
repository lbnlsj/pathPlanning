# 类定义

类 Actor:
    初始化(状态维度, 动作维度):
        创建三层神经网络: 
            输入层 -> 64节点
            隐藏层 -> 64节点
            输出层 -> 动作维度节点
    
    前向传播(状态):
        通过三层网络处理状态
        返回经过softmax的动作概率分布

类 Critic:
    初始化(状态维度, 动作维度, 智能体数量):
        创建三层神经网络:
            输入层(状态+动作) -> 64节点
            隐藏层 -> 64节点
            输出层 -> 1节点(Q值)
    
    前向传播(状态, 动作):
        将状态和动作连接
        通过三层网络处理
        返回Q值

类 Memory:
    初始化(最大容量):
        创建经验回放缓冲区
    
    存储(状态, 动作, 奖励, 下一状态, 完成标志):
        将经验元组存入缓冲区
    
    采样(批量大小):
        随机从缓冲区采样指定数量的经验
        返回批量经验数据

类 EMADDPG:
    初始化(状态维度, 动作维度, 智能体数量):
        为每个智能体创建:
            Actor网络
            Critic网络
            目标Actor网络
            目标Critic网络
            优化器
        初始化经验回放缓冲区
        设置超参数(学习率, 折扣因子, 软更新参数等)
    
    选择动作(状态):
        对每个智能体:
            使用Actor网络预测动作概率
            根据概率采样动作
        返回所有智能体的动作
    
    更新网络():
        如果经验不足则返回
        从经验回放中采样批量数据
        
        对每个智能体:
            # 更新Critic
            计算目标Q值:
                使用目标Actor预测下一个动作
                使用目标Critic计算目标Q值
                应用折扣因子和奖励
            
            计算当前Q值
            计算Critic损失
            更新Critic网络
            
            # 更新Actor
            计算Actor损失(基于Critic评估)
            更新Actor网络
            
            # 软更新目标网络
            缓慢更新目标Actor和Critic网络参数

# 训练流程
函数 训练(环境, MADDPG智能体, 回合数, 最大步数):
    循环 回合数 次:
        重置环境
        当前回合奖励 = 0
        
        循环 最大步数 次:
            获取当前状态
            选择动作
            执行动作获取下一状态、奖励和完成标志
            存储经验
            更新网络
            更新当前状态
            累加奖励
            
            如果回合结束则跳出
        
        输出回合信息

# 评估流程
函数 评估(环境, MADDPG智能体, 回合数, 最大步数):
    循环 回合数 次:
        重置环境
        当前回合奖励 = 0
        
        循环 最大步数 次:
            获取当前状态
            选择动作(无探索)
            执行动作获取下一状态、奖励和完成标志
            更新当前状态
            累加奖励
            渲染环境
            
            如果回合结束则跳出
        
        输出评估结果