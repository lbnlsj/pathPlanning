# 分层结构定义

类 GraphConvLayer:
    初始化(输入维度, 输出维度):
        创建权重矩阵
        创建偏置项
    
    前向传播(输入, 邻接矩阵):
        应用图卷积操作
        返回处理后的特征

类 GraphAttention:
    初始化(特征维度, 注意力头数):
        创建注意力权重
        创建特征变换矩阵
    
    前向传播(输入, 邻接矩阵):
        计算注意力分数
        聚合邻居信息
        返回注意力加权的特征

类 LayeredActor(Actor):
    初始化(状态维度, 动作维度):
        父类初始化(状态维度, 动作维度)
        创建图卷积层1
        创建图卷积层2
        创建图注意力层
        创建三层神经网络:
            输入层(图特征) -> 64节点
            隐藏层 -> 64节点
            输出层 -> 动作维度节点
    
    前向传播(状态, 邻接矩阵):
        图特征 = 图卷积层1(状态, 邻接矩阵)
        图特征 = 图卷积层2(图特征, 邻接矩阵)
        注意力特征 = 图注意力层(图特征, 邻接矩阵)
        组合特征 = 连接[图特征, 注意力特征]
        通过三层网络处理组合特征
        返回经过softmax的动作概率分布

类 LayeredCritic(Critic):
    初始化(状态维度, 动作维度, 智能体数量):
        父类初始化(状态维度, 动作维度, 智能体数量)
        创建图卷积层1
        创建图卷积层2
        创建图注意力层
        创建三层神经网络:
            输入层(图特征+动作) -> 64节点
            隐藏层 -> 64节点
            输出层 -> 1节点(Q值)
    
    前向传播(状态, 动作, 邻接矩阵):
        图特征 = 图卷积层1(状态, 邻接矩阵)
        图特征 = 图卷积层2(图特征, 邻接矩阵)
        注意力特征 = 图注意力层(图特征, 邻接矩阵)
        组合特征 = 连接[图特征, 注意力特征, 动作]
        通过三层网络处理组合特征
        返回Q值

类 LayeredEMADDPG(EMADDPG):
    初始化(状态维度, 动作维度, 智能体数量):
        为每个智能体创建:
            LayeredActor网络
            LayeredCritic网络
            目标LayeredActor网络
            目标LayeredCritic网络
            优化器
        初始化经验回放缓冲区
        设置超参数(学习率, 折扣因子, 软更新参数等)
    
    创建邻接矩阵(状态):
        计算智能体间距离矩阵
        根据距离阈值生成邻接关系
        归一化邻接矩阵
        返回邻接矩阵
    
    选择动作(状态):
        创建邻接矩阵
        对每个智能体:
            使用LayeredActor网络预测动作概率
            根据概率采样动作
        返回所有智能体的动作
    
    更新网络():
        如果经验不足则返回
        从经验回放中采样批量数据
        
        创建当前邻接矩阵和下一步邻接矩阵
        
        对每个智能体:
            # 更新Critic
            计算目标Q值:
                使用目标LayeredActor预测下一个动作
                使用目标LayeredCritic计算目标Q值
                应用折扣因子和奖励
            
            计算当前Q值
            计算Critic损失
            更新Critic网络
            
            # 更新Actor
            计算Actor损失(基于LayeredCritic评估)
            更新Actor网络
            
            # 软更新目标网络
            缓慢更新目标LayeredActor和LayeredCritic网络参数

# 修改后的训练流程
函数 分层训练(环境, LayeredMADDPG智能体, 回合数, 最大步数):
    循环 回合数 次:
        重置环境
        当前回合奖励 = 0
        
        循环 最大步数 次:
            获取当前状态
            创建邻接矩阵
            选择动作
            执行动作获取下一状态、奖励和完成标志
            存储经验(包含邻接矩阵)
            更新网络
            更新当前状态
            累加奖励
            
            如果回合结束则跳出
        
        输出回合信息

# 修改后的评估流程
函数 分层评估(环境, LayeredMADDPG智能体, 回合数, 最大步数):
    循环 回合数 次:
        重置环境
        当前回合奖励 = 0
        
        循环 最大步数 次:
            获取当前状态
            创建邻接矩阵
            选择动作(无探索)
            执行动作获取下一状态、奖励和完成标志
            更新当前状态
            累加奖励
            渲染环境
            
            如果回合结束则跳出
        
        输出评估结果