这篇论文使用的多智能体算法是**E-MADDPG（高效多智能体深度确定性策略梯度算法）**，这是对标准MADDPG算法的改进，专门用于解决大规模人群疏散中的路径规划问题。以下是该算法的详细介绍：

### 1. **MADDPG算法概述**
**MADDPG（Multi-Agent Deep Deterministic Policy Gradient，多智能体深度确定性策略梯度算法）**是一种适用于连续动作空间的策略梯度方法，专门用于多智能体环境。MADDPG使用集中训练、分布式执行的策略，训练时每个智能体的Critic（评价网络）会考虑所有智能体的状态和动作信息，而执行时每个智能体的Actor（策略网络）只使用自身的观察信息。这种机制允许智能体学习合作或竞争行为。

### 2. **E-MADDPG的改进**
E-MADDPG算法在MADDPG的基础上做了几项关键改进，以提高在人群疏散等复杂环境中的表现：


- **动态容量经验池**：MADDPG使用固定大小的经验池，而E-MADDPG引入了动态增长的经验池。随着算法学习的进行，经验池的大小会逐渐增大，从而允许智能体访问更多的多样化经验，提高学习效率。

- **优先级经验回放**：为了进一步提高学习效率，E-MADDPG采用了优先级经验回放策略。与随机从经验池中抽样不同，优先级回放会更频繁地重放对学习有较大帮助的经验（如高TD误差的经验），这使得智能体可以更快地学习关键经验，加速学习过程。

- **学习曲线机制**：E-MADDPG结合了学习曲线，根据学习的进展动态调整经验池的大小和采样的数量。随着经验的增长，算法会分配更多的资源用于训练，进一步提升学习能力。

### 3. **两层路径规划方法**
该论文提出了一种分层路径规划方法，将人群分为**领导者**和**跟随者**：
- **领导者**负责使用E-MADDPG算法进行路径规划，找到最优的疏散路径。
- **跟随者**使用**相对速度障碍（RVO）算法**避免碰撞，跟随领导者疏散。

#### a. **上层路径规划（领导者）**
领导者通过E-MADDPG算法进行路径规划。E-MADDPG通过强化学习方式，基于以下奖励函数来选择最优路径：
   - **距离目标出口的距离**：智能体离出口越近，奖励越高。
   - **避免碰撞**：与其他智能体或障碍物碰撞会受到惩罚。

领导者根据学习到的策略选择安全且最快的疏散路径，跟随者会沿着该路径进行疏散。

#### b. **下层避碰（跟随者）**
跟随者使用RVO算法避免与其他智能体的碰撞。RVO通过调整个体的速度，预测并避免在未来发生的碰撞，使整个疏散过程更加顺利。

### 4. **奖励函数**
E-MADDPG算法的奖励函数设计对引导智能体行为至关重要。奖励函数包括：
- **到达目标**：智能体接近出口时会获得正向奖励。
- **避免碰撞**：碰撞（无论是与其他智能体还是障碍物）会导致负向奖励。

通过结合这两项目标，E-MADDPG算法确保了智能体能够快速且安全地到达出口。

### 5. **算法步骤**
E-MADDPG的算法过程如下：
1. **初始化网络**：为每个智能体初始化Actor和Critic网络，并初始化经验回放池。
2. **领导者路径规划**：领导者通过策略网络选择动作，探索路径，成功的路径序列被存储。
3. **跟随者避碰**：跟随者使用RVO算法避开障碍物，同时跟随领导者的路径。
4. **经验回放和学习**：将经验存储在动态增长的经验池中，并通过优先级回放加速重要经验的学习。
5. **网络更新**：通过从经验池中采样的经验更新Actor和Critic网络。
6. **重复训练**：这一过程持续进行，直到智能体学会最优疏散路径。

### 6. **E-MADDPG的优势**
E-MADDPG算法在以下几个方面有显著优势：
- **更高的学习效率**：动态经验池和优先级经验回放加快了收敛速度，使智能体能够在复杂环境中更快地学习。
- **扩展性**：该算法能够处理大规模状态空间和多个智能体，适用于大规模的模拟场景（如人群疏散）。
- **避碰能力强**：RVO算法的引入确保了智能体在密集人群中避免碰撞，提升了疏散的安全性。

### 7. **应用场景**
E-MADDPG算法特别适用于以下场景：
- **人群疏散**：该算法旨在处理实时大规模人群疏散问题，确保人员高效、安全地撤离。
- **动态环境中的路径规划**：该算法可应用于任何多智能体系统，特别是需要路径规划和避碰的场景，如自动驾驶、机器人群体任务和游戏AI。

总结来说，该论文中的E-MADDPG算法是在MADDPG的基础上进行了多项改进，结合了动态经验池、优先级经验回放等机制，并通过分层的路径规划方法来提高在人群疏散等复杂场景中的效率和安全性。